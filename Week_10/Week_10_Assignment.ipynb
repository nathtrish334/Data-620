{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0GDbEofzN2Rz"
   },
   "source": [
    "\n",
    "# Week 10 - Document Classification\n",
    "\n",
    "**Trishita Nath**\n",
    "\n",
    "## Overview\n",
    "\n",
    "It can be useful to be able to classify new \"test\" documents using already classified \"training\" documents.  A common example is using a corpus of labeled spam and ham (non-spam) e-mails to predict whether or not a new document is spam.  Here is one example of such data:  [UCI Machine Learning Repository: Spambase Data Set](http://archive.ics.uci.edu/ml/datasets/Spambase)\n",
    "\n",
    "For this project, you can either use the above dataset to predict the class of new documents (either withheld from the training dataset or from another source such as your own spam folder).\n",
    "\n",
    "For more adventurous students, you are welcome (encouraged!) to come up a different set of documents (including scraped web pages!?) that have already been classified (e.g. tagged), then analyze these documents to predict how new documents should be classified.\n",
    "This assignment is due end of day on Sunday 4/17.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "gPpSLjlQQK_y",
    "outputId": "8f98adf9-6066-4bc3-975d-0e057e69c728"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package gutenberg to\n",
      "[nltk_data]     C:\\Users\\Happy\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\gutenberg.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['austen-emma.txt',\n",
       " 'austen-persuasion.txt',\n",
       " 'austen-sense.txt',\n",
       " 'bible-kjv.txt',\n",
       " 'blake-poems.txt',\n",
       " 'bryant-stories.txt',\n",
       " 'burgess-busterbrown.txt',\n",
       " 'carroll-alice.txt',\n",
       " 'chesterton-ball.txt',\n",
       " 'chesterton-brown.txt',\n",
       " 'chesterton-thursday.txt',\n",
       " 'edgeworth-parents.txt',\n",
       " 'melville-moby_dick.txt',\n",
       " 'milton-paradise.txt',\n",
       " 'shakespeare-caesar.txt',\n",
       " 'shakespeare-hamlet.txt',\n",
       " 'shakespeare-macbeth.txt',\n",
       " 'whitman-leaves.txt']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "import random\n",
    "random.seed(250)\n",
    "import pandas as pd\n",
    "pd.set_option('display.max_rows', 100)\n",
    "\n",
    "nltk.download('gutenberg')\n",
    "nltk.corpus.gutenberg.fileids() # Available texts in the guttenberg corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Gsl821Wl5JgI"
   },
   "source": [
    "### Comparison of the texts for Blake and Austen"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OHTGvw5Q5ZOL"
   },
   "source": [
    "#### Creating Texts\n",
    "\n",
    "\n",
    "\n",
    "Jane Austen has 3 books while Blake has one. I will take all three of Austen's works and combine them to create one text. Each author has their own style. I will remove punctuations and convert the text to lowercase and eliminate duplicate words. I will then create a list of text segments, each segment having 1000 words.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "aoY7Bn7Z6Jdl",
    "outputId": "f7393398-83ef-4bd4-b468-1a119aed5d86"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "366"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "austen_text = nltk.corpus.gutenberg.words('austen-emma.txt')+nltk.corpus.gutenberg.words('austen-persuasion.txt')+nltk.corpus.gutenberg.words('austen-sense.txt')\n",
    "austen_text = [word.lower() for word in austen_text if word.isalpha()]\n",
    "austen_segmented=[]\n",
    "for i in range(366):\n",
    "    austen_segmented.append([austen_text[i*1000:(i+1)*1000],'au'])\n",
    "len(austen_segmented)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "asfcIPN8AM37"
   },
   "source": [
    "We have a list of 432 1000-word segments of text by Jane Austen.\n",
    "\n",
    "I will do the same for the text by Blake, but this time, I will create text segments each having 990 words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-QCgGkZQAooZ",
    "outputId": "5d30d15b-0f6e-49d9-f82a-8a6a5f30fbbd"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "blake_text = nltk.corpus.gutenberg.words('blake-poems.txt')\n",
    "blake_text = [word.lower() for word in blake_text if word.isalpha()]\n",
    "blake_segmented=[]\n",
    "for i in range(7):\n",
    "    blake_segmented.append([blake_text[i*990:(i+1)*990],'bl'])\n",
    "len(blake_segmented)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1Ikr7RNHAxTr"
   },
   "source": [
    "\n",
    "\n",
    "We have a list of seven 990-word segments of text by William Blake."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "k9zyQrLIBDI0"
   },
   "source": [
    "### Feature exctraction\n",
    "\n",
    "I will take the two original lists of words and combine them into one list and find the 2000 most frequent words that I will use to create a feature list for my classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 424
    },
    "id": "yizxEX3LBZ4d",
    "outputId": "271810bd-be46-483f-89e0-e1bc061d6813"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>200 words</th>\n",
       "      <th>200 words</th>\n",
       "      <th>200 words</th>\n",
       "      <th>200 words</th>\n",
       "      <th>200 words</th>\n",
       "      <th>200 words</th>\n",
       "      <th>200 words</th>\n",
       "      <th>200 words</th>\n",
       "      <th>200 words</th>\n",
       "      <th>200 words</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>the</td>\n",
       "      <td>feelings</td>\n",
       "      <td>true</td>\n",
       "      <td>clay</td>\n",
       "      <td>purpose</td>\n",
       "      <td>smiled</td>\n",
       "      <td>estate</td>\n",
       "      <td>companions</td>\n",
       "      <td>sensibility</td>\n",
       "      <td>morton</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>to</td>\n",
       "      <td>found</td>\n",
       "      <td>agreeable</td>\n",
       "      <td>benwick</td>\n",
       "      <td>assured</td>\n",
       "      <td>thoroughly</td>\n",
       "      <td>run</td>\n",
       "      <td>suit</td>\n",
       "      <td>heartily</td>\n",
       "      <td>chiefly</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>and</td>\n",
       "      <td>few</td>\n",
       "      <td>taken</td>\n",
       "      <td>temper</td>\n",
       "      <td>extraordinary</td>\n",
       "      <td>enscombe</td>\n",
       "      <td>totally</td>\n",
       "      <td>fast</td>\n",
       "      <td>cruel</td>\n",
       "      <td>selfishness</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>of</td>\n",
       "      <td>heart</td>\n",
       "      <td>state</td>\n",
       "      <td>isabella</td>\n",
       "      <td>write</td>\n",
       "      <td>desirable</td>\n",
       "      <td>shewed</td>\n",
       "      <td>pressed</td>\n",
       "      <td>relation</td>\n",
       "      <td>turns</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>a</td>\n",
       "      <td>does</td>\n",
       "      <td>conversation</td>\n",
       "      <td>curiosity</td>\n",
       "      <td>ease</td>\n",
       "      <td>seat</td>\n",
       "      <td>line</td>\n",
       "      <td>dark</td>\n",
       "      <td>buildings</td>\n",
       "      <td>animated</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>195</th>\n",
       "      <td>till</td>\n",
       "      <td>themselves</td>\n",
       "      <td>letters</td>\n",
       "      <td>staying</td>\n",
       "      <td>suffer</td>\n",
       "      <td>eight</td>\n",
       "      <td>held</td>\n",
       "      <td>performance</td>\n",
       "      <td>distressed</td>\n",
       "      <td>endeavouring</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>196</th>\n",
       "      <td>something</td>\n",
       "      <td>within</td>\n",
       "      <td>spite</td>\n",
       "      <td>arrived</td>\n",
       "      <td>human</td>\n",
       "      <td>possibly</td>\n",
       "      <td>probable</td>\n",
       "      <td>furniture</td>\n",
       "      <td>explain</td>\n",
       "      <td>idle</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>197</th>\n",
       "      <td>dashwood</td>\n",
       "      <td>walk</td>\n",
       "      <td>sweet</td>\n",
       "      <td>suffered</td>\n",
       "      <td>hall</td>\n",
       "      <td>secure</td>\n",
       "      <td>add</td>\n",
       "      <td>earlier</td>\n",
       "      <td>filled</td>\n",
       "      <td>pursuits</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>198</th>\n",
       "      <td>yet</td>\n",
       "      <td>already</td>\n",
       "      <td>favour</td>\n",
       "      <td>consciousness</td>\n",
       "      <td>wallis</td>\n",
       "      <td>hearted</td>\n",
       "      <td>arise</td>\n",
       "      <td>musical</td>\n",
       "      <td>cloud</td>\n",
       "      <td>anger</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>199</th>\n",
       "      <td>look</td>\n",
       "      <td>year</td>\n",
       "      <td>weather</td>\n",
       "      <td>depend</td>\n",
       "      <td>gentle</td>\n",
       "      <td>property</td>\n",
       "      <td>possession</td>\n",
       "      <td>inn</td>\n",
       "      <td>berkeley</td>\n",
       "      <td>scruples</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>200 rows Ã— 10 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     200 words   200 words     200 words      200 words      200 words  \\\n",
       "0          the    feelings          true           clay        purpose   \n",
       "1           to       found     agreeable        benwick        assured   \n",
       "2          and         few         taken         temper  extraordinary   \n",
       "3           of       heart         state       isabella          write   \n",
       "4            a        does  conversation      curiosity           ease   \n",
       "..         ...         ...           ...            ...            ...   \n",
       "195       till  themselves       letters        staying         suffer   \n",
       "196  something      within         spite        arrived          human   \n",
       "197   dashwood        walk         sweet       suffered           hall   \n",
       "198        yet     already        favour  consciousness         wallis   \n",
       "199       look        year       weather         depend         gentle   \n",
       "\n",
       "      200 words   200 words    200 words    200 words     200 words  \n",
       "0        smiled      estate   companions  sensibility        morton  \n",
       "1    thoroughly         run         suit     heartily       chiefly  \n",
       "2      enscombe     totally         fast        cruel   selfishness  \n",
       "3     desirable      shewed      pressed     relation         turns  \n",
       "4          seat        line         dark    buildings      animated  \n",
       "..          ...         ...          ...          ...           ...  \n",
       "195       eight        held  performance   distressed  endeavouring  \n",
       "196    possibly    probable    furniture      explain          idle  \n",
       "197      secure         add      earlier       filled      pursuits  \n",
       "198     hearted       arise      musical        cloud         anger  \n",
       "199    property  possession          inn     berkeley      scruples  \n",
       "\n",
       "[200 rows x 10 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "austen_blake_combined = austen_text + blake_text\n",
    "all_words = nltk.FreqDist(w.lower() for w in austen_blake_combined)\n",
    "word_features = list(all_words)[:2000] \n",
    "\n",
    "word_list = []\n",
    "for i in range(0, 2000, 200):\n",
    "    df = pd.DataFrame(word_features[i:(i+200)])\n",
    "    df.columns=['200 words']\n",
    "    word_list.append(df)\n",
    "\n",
    "pd.concat(word_list, axis=1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "NqZSi0hTB4BO"
   },
   "outputs": [],
   "source": [
    "# Feature generator function that indicates whether or not each word is present in the text as a feature\n",
    "def document_features(document): \n",
    "    document_words = set(document) \n",
    "    features = {}\n",
    "    for word in word_features:\n",
    "        features['contains({})'.format(word)] = (word in document_words)\n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "dFYVN_I1CGj8",
    "outputId": "09b6afa5-c632-4102-a42e-64450ec28810"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('contains(the)', True),\n",
       " ('contains(to)', True),\n",
       " ('contains(and)', True),\n",
       " ('contains(of)', True),\n",
       " ('contains(a)', True),\n",
       " ('contains(i)', True),\n",
       " ('contains(her)', True),\n",
       " ('contains(in)', True),\n",
       " ('contains(was)', True),\n",
       " ('contains(it)', True)]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Testing on Austen's list\n",
    "features = document_features(austen_text)\n",
    "list(features.items())[:10]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Z2NHXsp60LLf"
   },
   "source": [
    "### Train and Test Dataset\n",
    "\n",
    "I will create a list of all text segments from both Austen and Blake then       shuffle them to create the text corpus that I will use to train and test the classifier model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ab7NfSxQ0NU-",
    "outputId": "4fcfca38-e279-4241-fcc3-8c15c839565e"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "373"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "documents=austen_segmented + blake_segmented\n",
    "\n",
    "random.shuffle(documents)\n",
    "feature_set = [(document_features(d), c) for (d,c) in documents]\n",
    "len(feature_set)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tYAuIbbi173U"
   },
   "source": [
    "#### Splitting the dataset into test and train datasets\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "BQTai-s82GSE",
    "outputId": "63cf11f1-bb33-4d5e-b096-71b742552d0e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9926739926739927\n"
     ]
    }
   ],
   "source": [
    "\n",
    "train_dataset, test_dataset = feature_set[:100], feature_set[100:]\n",
    "classifier = nltk.NaiveBayesClassifier.train(train_dataset)\n",
    "\n",
    "# Accuracy\n",
    "print(nltk.classify.accuracy(classifier, test_dataset)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3IVntUKKRv87",
    "outputId": "0cf4354b-1701-4fea-e384-49f1c3a99791"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most Informative Features\n",
      "        contains(bright) = True               bl : au     =     55.0 : 1.0\n",
      "         contains(green) = True               bl : au     =     55.0 : 1.0\n",
      "          contains(thou) = True               bl : au     =     55.0 : 1.0\n",
      "         contains(angel) = True               bl : au     =     33.0 : 1.0\n",
      "          contains(song) = True               bl : au     =     33.0 : 1.0\n",
      "         contains(which) = False              bl : au     =     33.0 : 1.0\n",
      "           contains(had) = False              bl : au     =     33.0 : 1.0\n",
      "           contains(her) = False              bl : au     =     33.0 : 1.0\n",
      "        contains(infant) = True               bl : au     =     33.0 : 1.0\n",
      "          contains(bore) = True               bl : au     =     23.6 : 1.0\n"
     ]
    }
   ],
   "source": [
    "classifier.show_most_informative_features(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SQRqbTMI6BaK"
   },
   "source": [
    "I will add two more authors to see the effect it has on the accuracy of the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ETKybh7cYeFf"
   },
   "source": [
    "#### Bugress Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "hdkHVrW6YiuI",
    "outputId": "53862189-c95a-4d05-db8f-a4d585a785b2"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "16327"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "burgess_text = nltk.corpus.gutenberg.words('burgess-busterbrown.txt')\n",
    "burgess_text = [word.lower() for word in burgess_text if word.isalpha()]\n",
    "burgess_segmented=[]\n",
    "for i in range(16):\n",
    "    burgess_segmented.append([burgess_text[i*1000:(i+1)*1000],'bu'])\n",
    "len(burgess_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "d8-E6QodYufs",
    "outputId": "73eb522d-08da-4678-be26-529069d53cf4"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "389"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "austen_blake_burgess_combined= austen_text + blake_text + burgess_text\n",
    "all_words = nltk.FreqDist(w.lower() for w in austen_blake_burgess_combined)\n",
    "word_features = list(all_words)[:2000] \n",
    "\n",
    "documents=austen_segmented + blake_segmented + burgess_segmented\n",
    "\n",
    "random.shuffle(documents)\n",
    "feature_sets = [(document_features(d), c) for (d,c) in documents]\n",
    "len(feature_sets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8M3yWJu_Y49W",
    "outputId": "540348a2-8b49-4463-df30-d5371c92c672"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9965397923875432\n"
     ]
    }
   ],
   "source": [
    "train_dataset, test_dataset = feature_sets[:100], feature_sets[100:]\n",
    "classifier = nltk.NaiveBayesClassifier.train(train_dataset)\n",
    "print(nltk.classify.accuracy(classifier, test_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "gXi8gifckjf9",
    "outputId": "6d1d1e3e-f18f-4329-92fa-dfa935efd889"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most Informative Features\n",
      "         contains(angel) = True               bl : au     =     50.0 : 1.0\n",
      "          contains(grey) = True               bl : au     =     50.0 : 1.0\n",
      "           contains(sun) = True               bl : au     =     50.0 : 1.0\n",
      "          contains(thou) = True               bl : au     =     50.0 : 1.0\n",
      "         contains(brown) = True               bu : au     =     45.0 : 1.0\n",
      "        contains(farmer) = True               bu : au     =     45.0 : 1.0\n",
      "         contains(brook) = True               bu : au     =     39.0 : 1.0\n",
      "             contains(m) = True               bu : au     =     33.0 : 1.0\n",
      "        contains(abroad) = True               bl : au     =     30.0 : 1.0\n",
      "        contains(breath) = True               bl : au     =     30.0 : 1.0\n"
     ]
    }
   ],
   "source": [
    "# Top 10 Most important features of the training our model\n",
    "\n",
    "classifier.show_most_informative_features(10)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SiOK8r2Zny71"
   },
   "source": [
    "#### Chesterson Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "jDDZS9pGn333",
    "outputId": "40ad2a91-57f8-407c-c5c4-bdd39689e26a"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "214692"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chesterson_text = nltk.corpus.gutenberg.words('chesterton-ball.txt')+nltk.corpus.gutenberg.words('chesterton-brown.txt')+nltk.corpus.gutenberg.words('chesterton-thursday.txt')\n",
    "chesterson_text = [word.lower() for word in chesterson_text if word.isalpha()]\n",
    "chesterson_segmented=[]\n",
    "for i in range(214):\n",
    "    chesterson_segmented.append([chesterson_text[i*1000:(i+1)*1000],'ch'])\n",
    "len(chesterson_text)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "_k5d4AnwoBlf",
    "outputId": "ccbe46b0-ebb9-4cad-95fc-39f8eba638ec"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "603"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "austen_blake_burgess_chesterson_combined = austen_text + blake_text + burgess_text + chesterson_text\n",
    "all_words = nltk.FreqDist(w.lower() for w in austen_blake_burgess_chesterson_combined)\n",
    "word_features = list(all_words)[:2000] \n",
    "\n",
    "documents=austen_segmented + blake_segmented + burgess_segmented + chesterson_segmented\n",
    "\n",
    "random.shuffle(documents)\n",
    "feature_sets = [(document_features(d), c) for (d,c) in documents]\n",
    "len(feature_sets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "W16qg6H3oHCk",
    "outputId": "aad12487-81f4-43fa-ea75-f2bb7a444d42"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9965397923875432\n"
     ]
    }
   ],
   "source": [
    "# Accuracy\n",
    "print(nltk.classify.accuracy(classifier, test_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "NQ7KL3Lduw-K",
    "outputId": "60471b3d-1d8e-47b0-dd27-39bbfecc9e27"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most Informative Features\n",
      "         contains(angel) = True               bl : au     =     50.0 : 1.0\n",
      "          contains(grey) = True               bl : au     =     50.0 : 1.0\n",
      "           contains(sun) = True               bl : au     =     50.0 : 1.0\n",
      "          contains(thou) = True               bl : au     =     50.0 : 1.0\n",
      "         contains(brown) = True               bu : au     =     45.0 : 1.0\n",
      "        contains(farmer) = True               bu : au     =     45.0 : 1.0\n",
      "         contains(brook) = True               bu : au     =     39.0 : 1.0\n",
      "             contains(m) = True               bu : au     =     33.0 : 1.0\n",
      "        contains(abroad) = True               bl : au     =     30.0 : 1.0\n",
      "        contains(breath) = True               bl : au     =     30.0 : 1.0\n"
     ]
    }
   ],
   "source": [
    "classifier.show_most_informative_features(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Week_10_Assignment.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
